//==============================================================================
// This file is part of the SPNC project under the Apache License v2.0 by the
// Embedded Systems and Applications Group, TU Darmstadt.
// For the full copyright and license information, please view the LICENSE
// file that was distributed with this source code.
// SPDX-License-Identifier: Apache-2.0
//==============================================================================

#include <llvm/Support/TargetRegistry.h>
#include "GPUtoLLVMConversion.h"
#include "mlir/Conversion/SCFToStandard/SCFToStandard.h"
#include "mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h"
#include "mlir/Conversion/GPUCommon/GPUCommonPass.h"
#include "mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h"
#include "mlir/Dialect/GPU/GPUDialect.h"
#include "mlir/InitAllPasses.h"
#include "mlir/Target/LLVMIR/ModuleTranslation.h"
#include "mlir/Translation.h"
#include "llvm/Support/TargetSelect.h"
#include "mlir/Target/NVVMIR.h"
#include "llvm/IRReader/IRReader.h"
#include "llvm/Linker/Linker.h"
#include "mlir/Dialect/StandardOps/Transforms/Passes.h"
#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
#include "llvm/Transforms/IPO/Internalize.h"
#include "llvm/ADT/SmallSet.h"
#include <driver/GlobalOptions.h>
#include <llvm/IR/LegacyPassManager.h>
#include "llvm/Target/TargetOptions.h"
#include "llvm/Target/TargetMachine.h"
#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/Transforms/IPO.h"
#include "llvm/Transforms/IPO/PassManagerBuilder.h"
#include "llvm/Transforms/Coroutines.h"

#ifndef SPNC_LIBDEVICE_FILE
// The location of libdevice is usually auto-detected and set by CMake.
#define SPNC_LIBDEVICE_FILE "/usr/local/cuda/nvvm/libdevice/libdevice.10.bc"
#endif

#include "cuda.h"

inline void emit_cuda_error(const llvm::Twine& message, const char* buffer,
                            CUresult error) {
  SPNC_FATAL_ERROR(message.concat(" failed with error code ")
                       .concat(llvm::Twine{error})
                       .concat("[")
                       .concat(buffer)
                       .concat("]").str());
}

#define RETURN_ON_CUDA_ERROR(expr, msg)                                        \
  {                                                                            \
    auto _cuda_error = (expr);                                                 \
    if (_cuda_error != CUDA_SUCCESS) {                                         \
      emit_cuda_error(msg, jitErrorBuffer, _cuda_error);                  \
      return {};                                                               \
    }                                                                          \
  }

mlir::OwnedBlob spnc::GPUtoLLVMConversion::compilePtxToCubin(const std::string& ptx, mlir::Location loc,
                                                             llvm::StringRef name) {
  // This code is mostly copy & pasta from mlir-cuda-runner.cpp

  // Text buffer to hold error messages if necessary.
  char jitErrorBuffer[4096] = {0};

  RETURN_ON_CUDA_ERROR(cuInit(0), "cuInit");

  // Linking requires a device context.
  CUdevice device = 0;
  RETURN_ON_CUDA_ERROR(cuDeviceGet(&device, 0), "cuDeviceGet");
  CUcontext context = nullptr;
  RETURN_ON_CUDA_ERROR(cuCtxCreate(&context, 0, device), "cuCtxCreate");
  CUlinkState linkState = nullptr;

  CUjit_option jitOptions[] = {CU_JIT_ERROR_LOG_BUFFER,
                               CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES};
  void* jitOptionsVals[] = {jitErrorBuffer,
                            reinterpret_cast<void*>(sizeof(jitErrorBuffer))};

  RETURN_ON_CUDA_ERROR(cuLinkCreate(2,              /* number of jit options */
                                    jitOptions,     /* jit options */
                                    jitOptionsVals, /* jit option values */
                                    &linkState),
                       "cuLinkCreate");

  // Add the PTX assembly generated by LLVM's PTX backend to the link modules.
  RETURN_ON_CUDA_ERROR(
      cuLinkAddData(linkState, CUjitInputType::CU_JIT_INPUT_PTX,
                    const_cast<void*>(static_cast<const void*>(ptx.c_str())),
                    ptx.length(), name.str().data(), /* kernel name */
                    0,                               /* number of jit options */
                    nullptr,                         /* jit options */
                    nullptr                          /* jit option values */
      ),
      "cuLinkAddData");

  void* cubinData = nullptr;
  size_t cubinSize = 0;
  RETURN_ON_CUDA_ERROR(cuLinkComplete(linkState, &cubinData, &cubinSize),
                       "cuLinkComplete");

  // Turn the generated Cubin into a binary blob that we can attach to the MLIR host module.
  char* cubinAsChar = static_cast<char*>(cubinData);
  mlir::OwnedBlob result =
      std::make_unique<std::vector<char>>(cubinAsChar, cubinAsChar + cubinSize);

  // This will also destroy the cubin data.
  RETURN_ON_CUDA_ERROR(cuLinkDestroy(linkState), "cuLinkDestroy");
  RETURN_ON_CUDA_ERROR(cuCtxDestroy(context), "cuCtxDestroy");

  return result;
}

void spnc::GPUtoLLVMConversion::linkWithLibdevice(llvm::Module* module, llvm::ArrayRef<llvm::StringRef> kernelFuncs) {
  // The kernel might use some optimized device functions from libdevice (__nv_*, e.g. __nv_exp).
  // libdevice is shipped as LLVM bitcode by Nvidia, so we load the bitcode file and link it
  // with the translated NVVM IR module.
  llvm::SMDiagnostic Err;
  auto libdevice = llvm::parseIRFile(SPNC_LIBDEVICE_FILE, Err, module->getContext());
  if (!libdevice) {
    SPNC_FATAL_ERROR("Failed to load libdevice: {}", Err.getMessage().str());
  }
  llvm::Linker::linkModules(*module, std::move(libdevice));

  // Internalize all functions except for the GPU kernel functions that were present in the module
  // before linking libdevice and conversion to NVVM.
  llvm::SmallSet<llvm::StringRef, 5> gpuKernels;
  gpuKernels.insert(kernelFuncs.begin(), kernelFuncs.end());
  llvm::internalizeModule(*module, [&gpuKernels](const llvm::GlobalValue& V) -> bool {
    if (gpuKernels.contains(V.getName())) {
      return true;
    }
    return false;
  });
}

void spnc::GPUtoLLVMConversion::optimizeNVVMIR(llvm::Module* module, const GPUOptimizationOptions& options) {
  // Set the nvvm-reflect-ftz flag to enable/disable use of fast-paths flushing subnormals to zero
  // during the NVVMReflect pass.
  module->addModuleFlag(llvm::Module::Override, "nvvm-reflect-ftz", options.flushDenormalsToZero);

  // Setup target machine.
  std::string errorMessage;
  auto target = llvm::TargetRegistry::lookupTarget("nvptx64-nvidia-cuda", errorMessage);
  if (!target) {
    SPNC_FATAL_ERROR("Failed to get target for NVPTX: {}", errorMessage);
  }
  std::unique_ptr<llvm::TargetMachine> machine(target->createTargetMachine("nvptx64-nvidia-cuda",
                                                                           options.computeCapability,
                                                                           options.features, {}, {}));

  // Create and populate pass manager.
  // This is copy & pasta code from OptUtils.cpp, with the important difference being that we let
  // the target machine adjust the pass manager, adding the NVVMReflectPass to our pass pipeline.
  llvm::legacy::PassManager modulePM;
  llvm::legacy::FunctionPassManager funcPM(module);
  llvm::PassManagerBuilder builder;
  builder.OptLevel = 3;
  builder.SizeLevel = 0;
  builder.Inliner = llvm::createFunctionInliningPass(3, 0, /*DisableInlineHotCallSite=*/false);
  builder.LoopVectorize = false; // Not required on GPU
  builder.SLPVectorize = false; // Not required on GPU
  builder.DisableUnrollLoops = false; // Allow loop unrolling.

  // Add all coroutine passes to the builder.
  llvm::addCoroutinePassesToExtensionPoints(builder);

  if (machine) {
    // Adjust the pass manager, which adds the NVVMReflectPass to the pipeline.
    machine->adjustPassManager(builder);
    // Add pass to initialize TTI for this specific target. Otherwise, TTI will
    // be initialized to NoTTIImpl by default.
    modulePM.add(createTargetTransformInfoWrapperPass(
        machine->getTargetIRAnalysis()));
    funcPM.add(createTargetTransformInfoWrapperPass(
        machine->getTargetIRAnalysis()));
  }

  builder.populateModulePassManager(modulePM);
  builder.populateFunctionPassManager(funcPM);
  funcPM.doInitialization();
  for (auto& func : *module) {
    funcPM.run(func);
  }
  funcPM.doFinalization();
  modulePM.run(*module);
}

std::unique_ptr<llvm::Module> spnc::GPUtoLLVMConversion::translateAndLinkGPUModule(llvm::ArrayRef<llvm::StringRef> kernelFuncs,
                                                                                   const GPUOptimizationOptions& options,
                                                                                   mlir::Operation* gpuModule,
                                                                                   llvm::LLVMContext& llvmContext,
                                                                                   llvm::StringRef name) {
  // Apply fast-math flags to all floating-point operations and functions in the GPU module.
  // TODO Reason about reassoc flag.
  auto fmf = mlir::LLVM::FastmathFlags::nsz | mlir::LLVM::FastmathFlags::contract |
      mlir::LLVM::FastmathFlags::arcp | mlir::LLVM::FastmathFlags::afn;
  gpuModule->walk([fmf](mlir::Operation* op) {
    if (auto fmi = mlir::dyn_cast<mlir::LLVM::FastmathFlagsInterface>(op)) {
      fmi->setAttr("fastmathFlags", mlir::LLVM::FMFAttr::get(fmf, fmi->getContext()));
    }
  });
  // Translate the input MLIR GPU module to NVVM IR (LLVM IR + some extension).
  auto llvmModule = mlir::translateModuleToNVVMIR(gpuModule, llvmContext, name);
  if (!llvmModule) {
    SPNC_FATAL_ERROR("Translation of GPU code to NVVM IR failed");
  }

  // Link the generated LLVM/NVVM IR with libdevice.
  linkWithLibdevice(llvmModule.get(), kernelFuncs);

  // Apply optimization passes to the LLVM/NVVM IR after linking with libdevice.
  optimizeNVVMIR(llvmModule.get(), options);

  if (options.printIRAfter) {
    llvm::dbgs() << "// *** IR Dump After conversion and optimization of NVVM IR ***\n\n";
    llvmModule->dump();
    llvm::dbgs() << "\n";
  }
  return llvmModule;
}

mlir::ModuleOp& spnc::GPUtoLLVMConversion::execute() {
  if (!cached) {
    // Initialize LLVM NVPTX backend, as we will lower the
    // content of the GPU module to PTX and compile it to cubin.
    LLVMInitializeNVPTXTarget();
    LLVMInitializeNVPTXTargetInfo();
    LLVMInitializeNVPTXTargetMC();
    LLVMInitializeNVPTXAsmPrinter();
    //
    // The lowering of the GPU-part of the input module involves the
    // following steps:
    // 1. Outline the GPU parts from the host part of the module.
    // 2. Run a bunch of transformation passes on the GPU-portion of the module.
    // 3. Convert the GPU kernels to a binary blob. For this purpose, the
    //    GPU portion of the module is translated to NVVM IR (essentially LLVM IR with some extensions)
    //    and compiled to PTX assembly using LLVM's PTX backend. The generated PTX is then compiled and
    //    linked into CUBIN using the CUDA runtime library API.
    //    The binary representation of the CUBIN is attached to the MLIR module as a
    //    string attribute and will be included as binary blob in the compiler output.
    //    At runtime, the binary blob is loaded with the CUDA API and executed.
    // 4. Apply the conversion to async calls to all the GPU calls on the host side.
    // 5. Replace the calls to GPU management functions on the host side with calls
    //    to a very thin runtime wrapper library around the CUDA API. This step also
    //    lowers the remaining code from standard to LLVM dialect.
    // 6. Lower the newly generated calls to LLVM dialect.
    // The result of this transformation is a MLIR module with only the host-part remaining as MLIR
    // code (the GPU portion is a binary blob attribute) in LLVM dialect that can then be lowered to LLVM IR.
    // Enable IR printing if requested via CLI

    // Setup the pass manager.
    mlir::PassManager pm{mlirContext.get()};
    if (spnc::option::dumpIR.get(*this->config)) {
      pm.enableIRPrinting(/* Print before every pass*/ [](mlir::Pass*, mlir::Operation*) { return false; },
          /* Print after every pass*/ [](mlir::Pass*, mlir::Operation*) { return true; },
          /* Print module scope*/ true,
          /* Print only after change*/ false);
    }
    auto inputModule = input.execute();
    // Clone the module to keep the original module available
    // for actions using the same input module.
    module = std::make_unique<mlir::ModuleOp>(inputModule.clone());

    // Collect all the GPU kernel function names present in the module
    // to preserve them during internalization.
    // This list is passed to translateAndLinkGPUModule.
    // NOTE: The GPU kernels should already have been outlined by
    // the GPUKernelOutliningPass.
    llvm::SmallVector<llvm::StringRef, 5> gpuKernels;
    module->walk([&gpuKernels](mlir::gpu::GPUFuncOp op) {
      gpuKernels.push_back(op.getName());
    });
    // Retrieve option value to pass to translateAndLinkGPUModule.
    auto printIR = spnc::option::dumpIR.get(*config);

    // Lower SCF constructs to CFG structure.
    pm.addPass(mlir::createLowerToCFGPass());
    // Nested pass manager operating only on the GPU-part of the code.
    auto& kernelPm = pm.nest<mlir::gpu::GPUModuleOp>();
    kernelPm.addPass(mlir::createStripDebugInfoPass());
    kernelPm.addPass(mlir::createLowerGpuOpsToNVVMOpsPass());
    // Convert the GPU-part to a binary blob and annotate it as an atttribute to the MLIR module.
    // translateAndLinkModule and compilePtxToCubin are call-backs.
    const char gpuBinaryAnnotation[] = "nvvm.cubin";
    auto gpuArch = getGPUArchitecture();
    GPUOptimizationOptions optimizationOptions{gpuArch, "+ptx60", true, printIR};
    kernelPm.addPass(mlir::createConvertGPUKernelToBlobPass(
        [&gpuKernels, &optimizationOptions](mlir::Operation* gpuModule,
                                                     llvm::LLVMContext& llvmContext,
                                                     llvm::StringRef name = "LLVMDialectModule") -> std::unique_ptr<llvm::Module> {
          return translateAndLinkGPUModule(gpuKernels, optimizationOptions, gpuModule, llvmContext, name);
        },
        GPUtoLLVMConversion::compilePtxToCubin,
        "nvptx64-nvidia-cuda", gpuArch, "+ptx60",
        gpuBinaryAnnotation));
    auto& funcPm = pm.nest<mlir::FuncOp>();
    funcPm.addPass(mlir::createStdExpandOpsPass());
    funcPm.addPass(mlir::createGpuAsyncRegionPass());
    funcPm.addPass(mlir::createAsyncRefCountingPass());
    // Convert the host-side GPU operations into runtime library calls.
    // This also lowers Standard-dialect operations to LLVM dialect.
    pm.addPass(mlir::createGpuToLLVMConversionPass(gpuBinaryAnnotation));
    pm.addPass(mlir::createAsyncToAsyncRuntimePass());
    pm.addPass(mlir::createConvertAsyncToLLVMPass());
    pm.addPass(mlir::createLowerToLLVMPass());

    auto result = pm.run(*module);
    if (failed(result)) {
      SPNC_FATAL_ERROR("Converting the GPU module failed");
    }
    auto verificationResult = module->verify();
    if (failed(verificationResult)) {
      SPNC_FATAL_ERROR("Module failed verification after conversion of GPU code");
    }
    cached = true;
  }
  return *module;
}

std::string spnc::GPUtoLLVMConversion::getGPUArchitecture() {
  // Text buffer to hold error messages if necessary.
  char jitErrorBuffer[4096] = {0};

  // Retrieve information about the compute capability of the GPU
  // hosted in this machine from the CUDA device driver API.
  // If multiple devices are present, arbitrarily choose the first
  // one to retrieve information from.

  RETURN_ON_CUDA_ERROR(cuInit(0), "cuInit");
  int numDevices = 0;
  RETURN_ON_CUDA_ERROR(cuDeviceGetCount(&numDevices), "cuDeviceGetCount");

  if (numDevices == 0) {
    SPDLOG_WARN("Found no CUDA devices, assuming architecture sm_35");
    return "sm_35";
  }
  SPDLOG_INFO("Found {} CUDA device(s)", numDevices);
  if (numDevices > 1) {
    SPDLOG_WARN("Found multiple CUDA devices, retrieving device information from first device");
  }
  CUdevice device = 0;
  RETURN_ON_CUDA_ERROR(cuDeviceGet(&device, 0), "cuDeviceGet");
  CUcontext context = nullptr;
  RETURN_ON_CUDA_ERROR(cuCtxCreate(&context, 0, device), "cuCtxCreate");
  char deviceName[1024] = {0};
  RETURN_ON_CUDA_ERROR(cuDeviceGetName(deviceName, sizeof(deviceName), device), "cuDeviceGetName");
  SPDLOG_INFO("Querying GPU device {} for information", deviceName);
  int major = 0;
  RETURN_ON_CUDA_ERROR(cuDeviceGetAttribute(&major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR,
                                            device), "cuDeviceGetAttribute (Compute capability major)");
  int minor = 0;
  RETURN_ON_CUDA_ERROR(cuDeviceGetAttribute(&minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR,
                                            device), "cuDeviceGetAttribute (Compute capability minor)");
  std::string arch = "sm_" + std::to_string(major) + std::to_string(minor);
  SPDLOG_INFO("GPU device {} supports compute capability {}", deviceName, arch);
  return arch;
}

spnc::GPUtoLLVMConversion::GPUtoLLVMConversion(ActionWithOutput<mlir::ModuleOp>& input,
                                               std::shared_ptr<mlir::MLIRContext> ctx) :
    ActionSingleInput<mlir::ModuleOp, mlir::ModuleOp>(input),
    mlirContext{std::move(ctx)} {}